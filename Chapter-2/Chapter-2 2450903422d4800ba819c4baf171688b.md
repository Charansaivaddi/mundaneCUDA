# Chapter-2

Date Created: August 4, 2025 11:16 AM
Status: In progress

# Data Parallelism:

Re-Organizing computation around the data such that we can execute resulting independent computations in parallel, which completes the overall job faster.

## Task Parallelism:

Decomposition of tasks into sub-tasks, each sub-task is executed independently.

Data parallelism is main source of scalability.

### Example

RGB Color Representation: Each pixel is stored as a tuple of (r, g, b) values. Each r, g, b belong to [0, 1] based on intensity o f respective color. (r, g, b) can be seen as (x, y, 1-x-y)

To  convert it to grey-scale, L = r*0.21 + g*0.72 + b*0.07 ⇒ luminance vale.

Input array with RGB values gives Output array of luminance values. None of per-pixel computation depends on each other, each conversion can be computed parallely.

# CUDA C

Terminology: 

Host : CPU

Device : GPU

Any C program is a CUDA program, considered as host-only code.

Device functions(targeted to GPU) and data declarations can be added to CUDA source file(program). These are clearly marked with CUDA C keywords.

Once these are added it is not accepted by C compiler, but compiled by NVCC(Nvidia c compiler). It processes source file by separating host code and device code.

host code is compiled by C/C++ compiler, device code which is marked with CUDA keywords for data-parallel functions called kernels.device code is compiled by NVCC and executed on GPU device. If no devices are found,  it is executed on CPU, kernels can be executed using tools like MCUDA.

When a kernel function is called or launched, is executed by a large number of threads. all threads that are generated by a launch collectively called grid. When all threads complete their respective tasks, grid terminates, the executions continues until another kernel is launched. 

Like in example, each thread is assigned to convert a pixel. IRL each thread may proces multple pixels. 

## Vector Addition Kernel

variable naming, “h_”, “d_” are for host, device respectively. 

Info: Vectors are stored in arrays A, B and output is stored in C. 

host code:

```c
void vecAdd(float* h_A, float* h_B, float* h_C, int n)
{
	for(int i=0; i<n; i++) h_C[i] = h_A[i] + h_B[i];
}

int main()
{
	vecAdd(h_A, h_B, h_C, N);
}
```

device code:

```c
#include <cuda.h>

void vecAdd(float* h_A, float* h_B, float* h_C, int n)
{
	float* d_A, d_B, d_c;
	//step-1: allocate device mem for A, B, C & copy A, B to device mem
	
	//step-2: kernel launch - vec addition is performed
	
	//step-3: copy C from device memory & free device vectors
}

```

cuda.h  -  a C preprocessor directive, it defines CUDA API functions & built-ins.

### Device Global Memory and Data Transfer

global memory aka device memory of gpu card is DRAM. Programmer allocates memory on device, transfers required data from host mem to gloabl mem, post task-completion device mem is brought back to host memory. 

Builtin-variables: predefined for programming language and given only-only access.

Allocating Device Memory: 

cudaMalloc() : most similar to C run-time malloc(),

1. Takes in **address of a pointer variable**, it will be set to allocated object. address should be casted to void** type since it expects a generic pointer.  It allows cudaMalloc() to to write address of allocated memory to pointer variable. Host code can oass this pointer value to kernel, to access the memory object.
2. 2nd parameter is **size of data** to be allocated(in bytes). 

cudaFree() : Frees memory object from device global memory.

1. **Pointer** of freed object.

```c
float* d_A;
int size = n;
cudaMalloc((void**)&d_A, size);
cudaFree(d_A);
```

cudaMemcpy(): Used to transfer allocated device memory from host to device. Takes in 4 parameters

1. **Pointer to destination** for data to to be copied.
2. **Pointer to source** location.
3. **Number of bytes** to be copied.
4. **Type/Direction of transfer** : host-to-host, host-ot-device, device-to-host, device-to-device.

In vecAdd case h_A, h_B from host to device, C from device to host.

```c
cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);
cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);
```

Stub function: Executed on host side, allocates device memory, requests data transfer and launches kernel.

```c
void vecAdd(float* h_A, float* h_B, float* h_C, int n)
{
	int size = n*sizeof(float);
	float *d_A, *d_B, *d_C;
	
	cudaMalloc((void**)&d_A, size);
	cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
	cudaMalloc((void**)&d_B, size);
	cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
	
	cudaMalloc((void**)&d_C, size);
	//kernel invocation
	cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
	
	//Free all
	cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
}
```

Best practices: 

```c
cudaError_t err = cudaMalloc((void**)&d_A, size);
if(error!=cudaSuccess){
	printf("%s in %s at line %d\n", cudaGetErrorString(err), __FILE__, __LINE__);
	exit(EXIT_FAILURE);
}
```

### Kernel Functions and Threading

In CUDA, kernel function defines the code to be executed by all threads. CUDA is an instance of SPMD(Single Program Multiple Data).

As host code launches kernel, CUDA run-time generates a grid of threads that are organized into 2-level hierarchy. Each grid is an array of thread blocks, which are called blocks here. All blocks of a grid are of same size. i.e., each block has 256 threads. Total no. of threads in a thread block can be specified in host code. For a grid no.of threads in a block is available in a built-in variable called blockDim. blockDim is a struct with 3 unsigned int fields: x, y, z and helps programmer organize threads in a 3D array. Choice of dimensionality of grid depends on dimensionality of data.

In general no. of threads in each dimension of thread block should be multiple of 32, for hardware efficiency reasons. 

Kernels have access to 2 variables: threadIdx, blockIdx - to distinguish among and to determine the area of data that allows thread to work on. threadIdx.x = value/index of thread in the block. blockIdx.x is common block coordinate for all threads in it. blockIdx, threadIdx used to create an unique global index ⇒ i = blockIdx.x*blockDim.x + threadIdx.x .

Usage of keyword __global__ : indicates the function is a **kernel function**, hence called from host code to generate grid of threads in device. Such functions are called from device and from host code only if CUDA system supports dynamic parallelism.

__device__ : the function is a CUDA **device function**. Executes on CUDA device and called from a Kernel function or another device function.

__host__ : the function is CUDA **host function**. It is a traditional C function, can be executed by host & called by a host function. All functions are host functions in CUDA unless any keyword is specified. 

A function can use both __host__ and __device__, it generates 2 versions of object files. 

| keyword | executed on | only callable from |
| --- | --- | --- |
| __device__ | device | device |
| __global__ | device | host |
| __host__ | host | host |

```c
__global__
void vecAddKernel(float* A, float* B, float* c, int n)
{
	int i = blockDim.x*blockIdx.x+threadIdx.x;
	if (i<n) C[i]=A[i]+B[i];
}
```

Though threads run same kernel code, they work on different data batches. threadIdx, blockIdx, blockDim are means for threads to access hardware registers. An automatic (local) variable i, which is private to each thread i.e., value i assigned to a thread is not visible other threads. 

Data parallelism is also referred as loop parallelism, where iterations of sequential code is executed by threads in parallel. 

Why i<n? Number of threads in a single block are multiples of 32, to process 100 length vector, it takes 4 blocks, with 28 more threads.

When host code launches a kernel, it sets a thread and block dimensions via execution configuration parameters.  can be given <<<ceil(n/256.0), 256>>>.

1st argument is number of thread blocks in grid. 2nd one is number of threads in a block. n/256 is found to compensate for all elements in the vector.

### Kernel Launch

```c
void vecAdd(float* A, float* B, float* C, int n)
{
	int size = n*sizeof(float);
	float *d_A, *d_B, *d_C;
	cudaMalloc((void**)&d_A, size);
	cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
	cudaMalloc((void**)&d_B, size);
	cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);
	
	cudaMalloc((void**)&d_C, size);
	
	vecAddKernel<<<ceil(n/256.0), 256>>>(d_A, d_B, d_C);
	cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);
	
	cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
}
```

Exercises:

1. To use each thread to calculate one output element.
    
    ```c
    i = blockIdx.x*blockDim.x + threadIdx.x
    ```
    
2. To use each thread to calculate two adjacent elements of vector addition
    
    ```c
    i = (blockIdx.x*blockDim.x + threadIdx.x) * 2
    // with changes which squentially handles 2 addition ops
    ```
    
3. Each block calculates 2*blockDim.x adjacent elements and move to next group.
    
    ```c
    i = blockIdx.x*blockDim.x*2 + threadIdx.x
    ```
    
4. Vector length = 8000, thread block size = 1024, each thread calculates one output. What is number of thread blocks to cover all the output elements.
    
    ceil(8000/1024.0) = 8 blocks, Total number of threads = 1024*8 = 8192
    
5. To allocates an array of v integer elements
    
    ```c
    int size = n * sizeof(int);
    ```
    
6. passing float*d_A, to cudaMalloc()
    
    ```c
    //cuda malloc takes a generic pointer
    cudaMalloc((void**)&d_A, size);
    ```
    
7. To copy 3000 bytes from host array h_A, to d_A
    
    ```c
    cudaMemcpy(d_A, h_A, 3000, cudaMemcpyHostToDevice);
    ```
    
8. How to declare a variable err
    
    ```c
    cudaError_t err;
    ```
    
9. How to declare function to execute on both host and device
    
    ```c
    __device__
    void vecAdd();
    ```